{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Eager Execution Enabled.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/zaidalyafeai/Notebooks/blob/master/Eager_Execution_Enabled.ipynb","timestamp":1540160246662}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"M7r1fJQniHj7","colab_type":"text"},"cell_type":"markdown","source":["# Introduction \n","\n","Eager Execution (EE) enables you to run operations immediately. As we know in TensorFlow, you have to create a graph and run it within a session in order to execute the operations of the graph. On the other hand, EE enables you to run operations directly and inspect the output as the operations are executed. This is very useful especially for debugging. Moreover, EE is pythonic and intergrates pretty well with numpy so it makes programming really easy and flexible. The next version of TenosrFlow \"2.0\" will enable EE by default. \n","\n","From Google AI [article](https://ai.googleblog.com/2017/10/eager-execution-imperative-define-by.html), these are some benefits of using EE\n","\n","   * Fast debugging with immediate run-time errors and integration with  python tools\n","   * Support for dynamic models using easy-to-use Python control flow\n","   * Strong support for custom and higher-order gradients\n","   * Almost all of the available TensorFlow operations\n","   \n","   ![alt text](https://i.imgur.com/YUlhihi.png)"]},{"metadata":{"id":"CiwZoGf85dnU","colab_type":"text"},"cell_type":"markdown","source":["# Enabling Eager Execution \n","In current versions of TensorFlow eager execution is not enabled by default so you have to enable it. "]},{"metadata":{"id":"jlnQG8hC-uCg","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","\n","tf.enable_eager_execution()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8Fq5Gb135Z5f","colab_type":"text"},"cell_type":"markdown","source":["Check if eager execution is enabled "]},{"metadata":{"id":"kS7er1hy-7yO","colab_type":"code","outputId":"5227d41b-3f1a-49f1-92a1-9405df8dbe04","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["tf.executing_eagerly()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"metadata":{"id":"UC6X5Y844_E-","colab_type":"text"},"cell_type":"markdown","source":["# Executing Ops Eagerly \n","By perfoming operations you can see the output directly without creating a session. "]},{"metadata":{"id":"DmwZJKlA_B15","colab_type":"code","outputId":"d1ec0347-5054-4acf-d110-f8dcc0d94802","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["x = [[2.]]\n","m = tf.square(x)\n","print(m)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n"],"name":"stdout"}]},{"metadata":{"id":"WGLoIPrQ6ZYT","colab_type":"text"},"cell_type":"markdown","source":["You can call `.numpy` to retrieve the results of the tensor as a numpy array"]},{"metadata":{"id":"-FGFGbZq6fRo","colab_type":"code","outputId":"d7378a89-55f6-42bb-ee2e-3b8db98e6fe2","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["m.numpy()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[4.]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":4}]},{"metadata":{"id":"l9Xlu7in6m22","colab_type":"text"},"cell_type":"markdown","source":["You can also compute an operation including two tensors "]},{"metadata":{"id":"d4tKJJ90_QMM","colab_type":"code","outputId":"92fb99d1-c57d-4ac4-8f18-1d405d9b60df","colab":{"base_uri":"https://localhost:8080/","height":69}},"cell_type":"code","source":["a = tf.constant([[1, 2],\n","                 [3, 4]])\n","\n","b = tf.constant([[2, 1],\n","                 [3, 4]])\n","\n","ab = tf.matmul(a, b)\n","\n","print('a * b = \\n', ab.numpy())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["a * b = \n"," [[ 8  9]\n"," [18 19]]\n"],"name":"stdout"}]},{"metadata":{"id":"V5qlVJygETcb","colab_type":"text"},"cell_type":"markdown","source":["# Constants and Variables \n","\n","\n","*   `tf.constant`, creates a constant tensor populated with the values as argument. The values are immutable. \n","*   `tf.Variable `, this method encapsultes a mutable tensor that can be changed later using assign \n"]},{"metadata":{"id":"ayMVXFj1FZxz","colab_type":"text"},"cell_type":"markdown","source":["Creating a constant tensor "]},{"metadata":{"id":"g2KFQKSLFNlS","colab_type":"code","outputId":"0b74dac1-9c3d-4a03-9f91-9f63dddb3886","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["a = tf.constant([[2,3]])\n","print(a)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tf.Tensor([[2 3]], shape=(1, 2), dtype=int32)\n"],"name":"stdout"}]},{"metadata":{"id":"mn8uX4t5FtHp","colab_type":"text"},"cell_type":"markdown","source":["A constant tensor is immutable so you cannot assign a new value to it."]},{"metadata":{"id":"xrJqeZfgHU-j","colab_type":"code","outputId":"ace27460-d9bd-480d-be05-f47962d1ae1b","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["try:\n","  a.assign([[3,4]])\n","except:\n","  print('Exception raised ')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Exception raised \n"],"name":"stdout"}]},{"metadata":{"id":"lrgbhCu8H3rm","colab_type":"text"},"cell_type":"markdown","source":["On the other hand variables are mutable and can be assigned a new value"]},{"metadata":{"id":"WSMIotOQFw2F","colab_type":"code","outputId":"adf3a704-f683-4d48-a0d5-7b3728f15d2f","colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["v = tf.Variable(5.)\n","\n","print('Old value for v =', v.numpy())\n","v.assign(2.)\n","print('New value for v =', v.numpy())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Old value for v = 5.0\n","New value for v = 2.0\n"],"name":"stdout"}]},{"metadata":{"id":"se0MFrEwMXWo","colab_type":"text"},"cell_type":"markdown","source":["You can also increment/decrement the value of a tensor "]},{"metadata":{"id":"d9M50PpdMzEn","colab_type":"code","outputId":"1c135c36-0a0d-4a0f-d885-6b8d7e53ffd9","colab":{"base_uri":"https://localhost:8080/","height":69}},"cell_type":"code","source":["v.assign(2.)\n","print('value     : ', v.numpy())\n","print('increment : ', tf.assign_add(v, 1).numpy())\n","print('decrement : ', tf.assign_sub(v, 1).numpy())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["value     :  2.0\n","increment :  3.0\n","decrement :  2.0\n"],"name":"stdout"}]},{"metadata":{"id":"FtGAYUUWI8bX","colab_type":"text"},"cell_type":"markdown","source":["You can return many information from a tensor variable, like the name, type , shape and what device it executes on. "]},{"metadata":{"id":"2wQMtx3QJBSg","colab_type":"code","outputId":"68daaa75-d2ca-4919-b250-3133f6eb22c9","colab":{"base_uri":"https://localhost:8080/","height":86}},"cell_type":"code","source":["print('name  : ', v.name)\n","print('type  : ', v.dtype)\n","print('shape : ', v.shape)\n","print('device: ', v.device)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["name  :  Variable:0\n","type  :  <dtype: 'float32'>\n","shape :  ()\n","device:  /job:localhost/replica:0/task:0/device:GPU:0\n"],"name":"stdout"}]},{"metadata":{"id":"5irSm-yDN0nV","colab_type":"text"},"cell_type":"markdown","source":["# Gradient Evaluation"]},{"metadata":{"id":"y36ig_TVAAoM","colab_type":"text"},"cell_type":"markdown","source":["Gradient evaluation is very importnat machine learning because it is based on function optimization. You can use `tf.GradientTape()` method to record the gradient of an arbitrary function"]},{"metadata":{"id":"bdZmXyAi_3M3","colab_type":"code","outputId":"e829b23f-62bd-4e8b-8e0f-654c2608766c","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["w = tf.Variable(2.0)\n","\n","#watch the gradient of the loss operation\n","with tf.GradientTape() as tape:\n","  loss = w * w\n","\n","grad = tape.gradient(loss, w)\n","print(f'The gradient of w^2 at {w.numpy()} is {grad.numpy()}')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The gradient of w^2 at 2.0 is 4.0\n"],"name":"stdout"}]},{"metadata":{"id":"qEMEMfV8Pwvt","colab_type":"text"},"cell_type":"markdown","source":["You also compute the gradient directly using `gradients_function`. In this example we evaluate the gradient of the sigmoid function \n","\n","$$f(x) = \\frac{1}{1+e^{-x}}$$\n","\n","Note that \n","\n","$$f'(x) = \\frac{e^{-x}}{(1+e^{-x})^2} = f(x)(1-f(x)) $$"]},{"metadata":{"id":"nrw-DuoWP0A6","colab_type":"code","outputId":"85df0b9c-d977-48e2-a866-26b86ddb8ba9","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["import tensorflow.contrib.eager as tfe \n","\n","def sigmoid(x):\n","  return 1/(1 + tf.exp(-x))\n","\n","grad_sigmoid = tfe.gradients_function(sigmoid)\n","\n","print('The gradient of the sigmoid function at 2.0 is ', grad_sigmoid(2.0)[0].numpy())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The gradient of the sigmoid function at 2.0 is  0.104993574\n"],"name":"stdout"}]},{"metadata":{"id":"Jew_BsZaYeVz","colab_type":"text"},"cell_type":"markdown","source":["You can also compute higher order derivatives by nesting a gradient functions. For instance, \n","\n","$$f(x) = \\log(x) , f'(x) = \\frac{1}{x}, f''(x) = \\frac{-1}{x^2}$$"]},{"metadata":{"id":"UoFFIr_AXUnm","colab_type":"code","outputId":"ea684bb1-e807-4a18-e1ff-ceae70d3884c","colab":{"base_uri":"https://localhost:8080/","height":69}},"cell_type":"code","source":["dx = tfe.gradients_function\n","\n","def log(x):\n","  return tf.log(x)\n","\n","dx_log = dx(log)\n","dx2_log = dx(dx(log))\n","dx3_log = dx(dx(dx(log)))\n","\n","print('The first  derivative of log at x = 1 is ', dx_log(1.)[0].numpy())\n","print('The second derivative of log at x = 1 is ', dx2_log(1.)[0].numpy())\n","print('The third  derivative of log at x = 1 is ', dx3_log(1.)[0].numpy())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The first  derivative of log at x = 1 is  1.0\n","The second derivative of log at x = 1 is  -1.0\n","The third  derivative of log at x = 1 is  2.0\n"],"name":"stdout"}]},{"metadata":{"id":"hBVnUE6DRDFw","colab_type":"text"},"cell_type":"markdown","source":["# Custom Gradients\n","\n","Some times the gradient is not what we want espeically if there is a problem in numerical instabilitiy. Consider the following function and its gradient \n","\n","$$f(x) = \\log(1+e^x)$$\n","\n","The gradient is \n","\n","$$f'(x) = \\frac{e^x}{1+e^x}$$\n","\n","Note that at big values of $x$ the gradient value will blow up."]},{"metadata":{"id":"autHEivlRp9M","colab_type":"code","outputId":"a5cfb0ef-0f4d-45a2-a301-d7388b6db2c8","colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["def logexp(x):\n","  return tf.log(1 + tf.exp(x))\n","grad_logexp = tfe.gradients_function(logexp)\n","\n","print('The gradient at x = 0  is ', grad_logexp(0.)[0].numpy())  \n","\n","print('The gradient at x = 100 is ', grad_logexp(100.)[0].numpy()) "],"execution_count":0,"outputs":[{"output_type":"stream","text":["The gradient at x = 0  is  0.5\n","The gradient at x = 100 is  nan\n"],"name":"stdout"}]},{"metadata":{"id":"c19m7XawUR1a","colab_type":"text"},"cell_type":"markdown","source":[" We can revaluate the gradient by overriding the gradient of the function. We can recompute the gradient as \n","\n","$$f(x) =  \\frac{1+e^x -e^x }{1+e^x} = 1 - \\frac{1}{1 + e^{x}}$$"]},{"metadata":{"id":"EmPT2S6XUJ8C","colab_type":"code","outputId":"74688ca7-dbfe-4bd1-b362-075ba0a87ae1","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["@tf.custom_gradient\n","def logexp_stable(x):\n","  e = tf.exp(x)\n","  #dy is optional, allows computation of vector jacobian products for vectors other than the vector of ones.\n","  def grad(dy):\n","    return dy * (1 - 1 / (1 + e))\n","  return tf.log(1 + e), grad\n","\n","grad_logexp_stable = tfe.gradients_function(logexp_stable)\n","\n","print('The gradient at x = 100 is ', grad_logexp_stable(100.)[0].numpy()) "],"execution_count":0,"outputs":[{"output_type":"stream","text":["The gradient at x = 100 is  1.0\n"],"name":"stdout"}]},{"metadata":{"id":"WhLnr59mdFh2","colab_type":"text"},"cell_type":"markdown","source":["# Execution Callbacks"]},{"metadata":{"id":"vF0MR4mPdduw","colab_type":"text"},"cell_type":"markdown","source":["`add_execution_callback` can be used to monitor the execution of operations. These functions will be called when any function is executed eagerly. In this example we record the operation names."]},{"metadata":{"id":"a3ybT2GPdLqU","colab_type":"code","outputId":"4dd781e6-9583-46b0-aca2-cc316d6d9f1a","colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["#create a callback that records the operation name \n","def print_op(op_type, op_name, attrs, inputs, outputs):\n","  print(op_type)\n","  \n","#clear previous callbacks\n","tfe.clear_execution_callbacks() \n","\n","#add the callback \n","tfe.add_execution_callback(print_op)\n","\n","#try runing an operation \n","x = tf.pow(2.0, 3.0) - 3.0\n","\n","#clear the callback \n","tfe.clear_execution_callbacks() "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Pow\n","Sub\n"],"name":"stdout"}]},{"metadata":{"id":"qofbAYzRO3IW","colab_type":"text"},"cell_type":"markdown","source":["# Object Oriented Metrics\n","You can use `metrics` to record tensors/values and operate on them at the end. This is useful when recording the training history and you want to evaluate it at the end. Use `.result()` to evaluate the metric at the end. "]},{"metadata":{"id":"u4DkG8BBO74o","colab_type":"code","outputId":"00ac56f7-f8f5-4cab-d0bd-f4f27024f5e4","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["m = tfe.metrics.Mean(\"loss\")\n","\n","#record the loss \n","m(2)\n","m(4)\n","\n","print('The mean loss is ', m.result().numpy())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The mean loss is  3.0\n"],"name":"stdout"}]},{"metadata":{"id":"erOTyJXWbHNJ","colab_type":"text"},"cell_type":"markdown","source":["If you want to remove the recorded values, you can reinstintiate the variables "]},{"metadata":{"id":"VJ20YHw9bILA","colab_type":"code","outputId":"216e2a62-f743-47d2-be56-248de06c9005","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["m.init_variables()\n","print('The mean loss is ', m.result().numpy())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The mean loss is  nan\n"],"name":"stdout"}]},{"metadata":{"id":"hbIHKtEEbujP","colab_type":"text"},"cell_type":"markdown","source":["# Linear Regression \n","\n","This example is refactored from https://www.tensorflow.org/guide/eager. We create a complete example of using linear regression to predict the paramters of the function \n","\n","$$f(x) = 3 x + 2 + noise$$\n","\n","Given a point $x$ we want to predict the value of $y$. We train the model on 1000 data pairs $(x,f(x))$. \n","\n","The model to learn is a linear model \n","\n","$$\\hat{y} = W x + b$$\n","\n","Note that, we use `tf.GradientTape` to record the gradient with respect our trainable paramters.  \n","\n","We MSE to calcuate the loss \n","\n","$$g = (y-\\hat{y})^2$$\n","\n","We use Gradient Descent to update the paramters \n","\n","$$W = W - \\alpha  \\frac{\\partial g}{\\partial W}$$\n","\n","$$b = b - \\alpha  \\frac{\\partial g}{\\partial b}$$"]},{"metadata":{"id":"P7A5lcylAERT","colab_type":"code","outputId":"bc72f562-bfb3-470c-855a-0754cb6ee968","colab":{"base_uri":"https://localhost:8080/","height":208}},"cell_type":"code","source":["#1000 data points \n","NUM_EXAMPLES = 1000\n","\n","#define inputs and outputs with some noise \n","X = tf.random_normal([NUM_EXAMPLES])  #inputs \n","noise = tf.random_normal([NUM_EXAMPLES]) #noise \n","y = X * 3 + 2 + noise  #true output\n","\n","#create model paramters with initial values \n","W = tf.Variable(0.)\n","b = tf.Variable(0.)\n","\n","#training info\n","train_steps = 200\n","learning_rate = 0.01\n","\n","for i in range(train_steps):\n","  \n","  #watch the gradient flow \n","  with tf.GradientTape() as tape:\n","    \n","    #forward pass \n","    yhat = X * W + b\n","    \n","    #calcuate the loss (difference squared error)\n","    error = yhat - y\n","    loss = tf.reduce_mean(tf.square(error))\n","  \n","  #evalute the gradient with the respect to the paramters\n","  dW, db = tape.gradient(loss, [W, b])\n","\n","  #update the paramters using Gradient Descent  \n","  W.assign_sub(dW * learning_rate)\n","  b.assign_sub(db* learning_rate)\n","\n","  #print the loss every 20 iterations \n","  if i % 20 == 0:\n","    print(\"Loss at step {:03d}: {:.3f}\".format(i, loss))\n","      \n","print(f'W : {W.numpy()} , b  = {b.numpy()} ')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loss at step 000: 14.478\n","Loss at step 020: 6.947\n","Loss at step 040: 3.632\n","Loss at step 060: 2.173\n","Loss at step 080: 1.531\n","Loss at step 100: 1.248\n","Loss at step 120: 1.124\n","Loss at step 140: 1.069\n","Loss at step 160: 1.045\n","Loss at step 180: 1.035\n","W : 2.9950246810913086 , b  = 1.9608360528945923 \n"],"name":"stdout"}]},{"metadata":{"id":"JE8BnpyA7kfZ","colab_type":"text"},"cell_type":"markdown","source":["# A Simple CNN\n","\n","Here we create a simple convolutional neural network (CNN)to recognize hand-written digits (MNIST). We start by creating a simple alexnet CNN model. \n","\n","![alt text](https://i.imgur.com/twUKrlo.png)"]},{"metadata":{"id":"5nJYdULqiee7","colab_type":"code","colab":{}},"cell_type":"code","source":["from tensorflow.keras.layers import Dense, Convolution2D, MaxPooling2D, Flatten, BatchNormalization, Dropout\n","from tensorflow.keras.models import Sequential"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_cuZWAmejXH4","colab_type":"code","colab":{}},"cell_type":"code","source":["def create_model():\n","  model = Sequential()\n","  model.add(Convolution2D(filters = 16, kernel_size = 3, padding = 'same', input_shape = [28, 28, 1], activation = 'relu'))\n","  model.add(MaxPooling2D(pool_size = (2,2)))\n","  model.add(BatchNormalization())\n","  model.add(Convolution2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\n","  model.add(MaxPooling2D(pool_size = (2,2)))\n","  model.add(BatchNormalization())\n","  model.add(Flatten())\n","  model.add(Dense(units = 100, activation = 'relu'))\n","  model.add(Dropout(0.5))\n","  model.add(Dense(units = 10 , activation = 'softmax'))\n","  return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YkEgDGCmO7_i","colab_type":"text"},"cell_type":"markdown","source":["## Create the model "]},{"metadata":{"id":"LvTw6rrnkiMI","colab_type":"code","outputId":"96280791-1bd5-468f-c48c-b19b2ad45278","colab":{"base_uri":"https://localhost:8080/","height":486}},"cell_type":"code","source":["model = create_model()\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_6 (Conv2D)            (None, 28, 28, 16)        160       \n","_________________________________________________________________\n","max_pooling2d_6 (MaxPooling2 (None, 14, 14, 16)        0         \n","_________________________________________________________________\n","batch_normalization_6 (Batch (None, 14, 14, 16)        64        \n","_________________________________________________________________\n","conv2d_7 (Conv2D)            (None, 14, 14, 32)        4640      \n","_________________________________________________________________\n","max_pooling2d_7 (MaxPooling2 (None, 7, 7, 32)          0         \n","_________________________________________________________________\n","batch_normalization_7 (Batch (None, 7, 7, 32)          128       \n","_________________________________________________________________\n","flatten_3 (Flatten)          (None, 1568)              0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 100)               156900    \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 100)               0         \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 10)                1010      \n","=================================================================\n","Total params: 162,902\n","Trainable params: 162,806\n","Non-trainable params: 96\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"w8xMs3nh5cw2","colab_type":"text"},"cell_type":"markdown","source":["Look at the output by frowarding a batch of zero images. "]},{"metadata":{"id":"ZilarnvZ5cDS","colab_type":"code","outputId":"b7a65d11-1c27-4364-bc67-9e78cfc994ed","colab":{"base_uri":"https://localhost:8080/","height":208}},"cell_type":"code","source":["import numpy as np\n","model(np.zeros((10, 28, 28, 1), np.float32))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: id=8234584, shape=(10, 10), dtype=float32, numpy=\n","array([[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n","       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n","       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n","       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n","       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n","       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n","       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n","       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n","       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n","       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":60}]},{"metadata":{"id":"ZSXNAsM0O9-E","colab_type":"text"},"cell_type":"markdown","source":["## Load MNIST dataset"]},{"metadata":{"id":"fQxLk3_Lk3OP","colab_type":"code","colab":{}},"cell_type":"code","source":["(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K00wu_RI547V","colab_type":"text"},"cell_type":"markdown","source":["Look at the data"]},{"metadata":{"id":"vAuOTfX756oz","colab_type":"code","outputId":"db631847-c7ec-49d7-98e6-c83bc530e787","colab":{"base_uri":"https://localhost:8080/","height":364}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","print('The label is ',y_train[0])\n","plt.imshow(x_train[0])\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The label is  5\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<matplotlib.figure.Figure at 0x7f3eba951c50>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"cWB0w1IfPAsu","colab_type":"text"},"cell_type":"markdown","source":["## Preprcocessing the dataset "]},{"metadata":{"id":"9iVSk1i_ldCG","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","\n","N = x_train.shape[0]\n","\n","#normalization and convert to batch input \n","x_train = tf.expand_dims(np.float32(x_train)/ 255., 3)\n","x_test  = tf.expand_dims(np.float32(x_test )/ 255., 3)\n","\n","#one hot encoding\n","y_train = tf.one_hot(y_train, 10)\n","y_test  = tf.one_hot(y_test , 10)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LhmXmozJPKuj","colab_type":"text"},"cell_type":"markdown","source":["## Get Random Batch"]},{"metadata":{"id":"QAtM7bV4lOgh","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","def get_batch(batch_size = 32):\n","  r = np.random.randint(0, N-batch_size)\n","  return x_train[r: r + batch_size], y_train[r: r + batch_size]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nIRZj1pyPnbJ","colab_type":"text"},"cell_type":"markdown","source":["## Design the loss function, Gradient and Accuracy metric"]},{"metadata":{"id":"6ERHfE-Z4KvV","colab_type":"code","colab":{}},"cell_type":"code","source":["#evaluate the loss\n","def loss(model, x, y):\n","  prediction = model(x)\n","  return tf.losses.softmax_cross_entropy(y, logits=prediction)\n","\n","#record the gradient with respect to the model variables \n","def grad(model, x, y):\n","  with tf.GradientTape() as tape:\n","    loss_value = loss(model, x, y)\n","  return tape.gradient(loss_value, model.variables)\n","\n","#calcuate the accuracy of the model \n","def accuracy(model, x, y):\n","  \n","  #prediction\n","  yhat = model(x)\n","  \n","  #get the labels of the predicted values \n","  yhat = tf.argmax(yhat, 1).numpy()\n","  \n","  #get the labels of the true values\n","  y    = tf.argmax(y   , 1).numpy()\n","  return np.sum(y == yhat)/len(y)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yw-Mk_nJP0vp","colab_type":"text"},"cell_type":"markdown","source":["## Intitalize the variables"]},{"metadata":{"id":"6UXs1mEKSRwe","colab_type":"code","colab":{}},"cell_type":"code","source":["i = 1 \n","\n","batch_size = 64 \n","epoch_length = N // batch_size\n","epoch = 0 \n","epochs = 5 \n","\n","#use Adam optimizer \n","optimizer = tf.train.AdamOptimizer()\n","\n","#record epoch loss and accuracy  \n","loss_history = tfe.metrics.Mean(\"loss\")\n","accuracy_history = tfe.metrics.Mean(\"accuracy\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Xm5mBuUOSQwg","colab_type":"text"},"cell_type":"markdown","source":["## Training"]},{"metadata":{"id":"XUx2dlTirg_t","colab_type":"code","outputId":"2f5128d8-e9c1-4a2f-8b05-8a6a7283c518","colab":{"base_uri":"https://localhost:8080/","height":104}},"cell_type":"code","source":["while epoch < epochs:\n","  #get next batch\n","  x, y = get_batch(batch_size = batch_size)\n","\n","  # Calculate derivatives of the input function with respect to its parameters.\n","  grads = grad(model, x, y)\n","\n","  # Apply the gradient to the model\n","  optimizer.apply_gradients(zip(grads, model.variables),\n","                            global_step=tf.train.get_or_create_global_step())\n","  \n","  #record the current loss and accuracy   \n","  loss_history(loss(model, x, y))\n","  accuracy_history(accuracy(model, x, y))\n","  \n","  if i % epoch_length == 0:\n","    print(\"epoch: {:d} Loss: {:.3f}, Acc: {:.3f}\".format(epoch, loss_history.result(), accuracy_history.result()))\n","    \n","    #clear the history \n","    loss_history.init_variables()\n","    accuracy_history.init_variables()\n","    \n","    epoch += 1\n","    \n","  i += 1"],"execution_count":0,"outputs":[{"output_type":"stream","text":["epoch: 0 Loss: 1.548, Acc: 0.922\n","epoch: 1 Loss: 1.484, Acc: 0.979\n","epoch: 2 Loss: 1.478, Acc: 0.985\n","epoch: 3 Loss: 1.474, Acc: 0.988\n","epoch: 4 Loss: 1.472, Acc: 0.990\n"],"name":"stdout"}]},{"metadata":{"id":"onYJgb64Yct-","colab_type":"text"},"cell_type":"markdown","source":["## Testing"]},{"metadata":{"id":"Xdk9lG-sB6Y5","colab_type":"code","outputId":"a7a8beae-6f2f-4827-abf2-736b797a146a","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["accuracy(model, x_test, y_test)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.986"]},"metadata":{"tags":[]},"execution_count":69}]},{"metadata":{"id":"VKtzhgMYNxFr","colab_type":"text"},"cell_type":"markdown","source":["# Save and Restore a Model\n","You can save training history then restore it later. "]},{"metadata":{"id":"Ci3mmDTLM7oI","colab_type":"code","outputId":"2f86240c-0d6a-4478-be35-5469c1b6e4ab","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["#create a directory for saving the model\n","import os \n","checkpoint_dir = 'model'\n","os.mkdir(checkpoint_dir)\n","\n","#create a root for the checkpoint\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","root = tf.train.Checkpoint(optimizer=optimizer,\n","                           model=model,\n","                           optimizer_step=tf.train.get_or_create_global_step())\n","\n","#save the model \n","root.save(file_prefix=checkpoint_prefix)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'model/ckpt-1'"]},"metadata":{"tags":[]},"execution_count":70}]},{"metadata":{"id":"SMkTMzQjUjYy","colab_type":"text"},"cell_type":"markdown","source":["Restore the model "]},{"metadata":{"id":"AHFYo3Z-UPG0","colab_type":"code","outputId":"40c0d0f9-36f1-4d82-a22e-39bc2dffa18e","colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["#create an empty model \n","model = create_model()\n","\n","#accuracy of the empty model \n","print('accuracy before retrieving the model ',accuracy(model, x_test, y_test))\n","\n","#create a checkpoint variable \n","root = tf.train.Checkpoint(optimizer=optimizer,\n","                           model=create_model(),\n","                           optimizer_step=tf.train.get_or_create_global_step())\n","\n","#restore the model\n","root.restore(tf.train.latest_checkpoint(checkpoint_dir))\n","\n","#retrieve the trained model \n","model = root.model \n","\n","print('accuracy after retrieving the model ',accuracy(model, x_test, y_test))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["accuracy before retrieving the model  0.096\n","accuracy after retrieving the model  0.986\n"],"name":"stdout"}]},{"metadata":{"id":"LeJDAuS0-eZq","colab_type":"text"},"cell_type":"markdown","source":["# Compiling Functions into Callable Graph\n","`defun`  trace-compiles a Python function composed of TensorFlow operations into a callable that executes a `tf.Graph` containing those operations."]},{"metadata":{"id":"K8v-qLzi_8YH","colab_type":"code","colab":{}},"cell_type":"code","source":["# A simple example.\n","def f(x):\n","  return tf.square(x)\n","\n","#callable graph function\n","g = tfe.defun(f)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cuC-reooBdIh","colab_type":"code","outputId":"32331119-e47b-4a33-938b-c8fcd6762832","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["x = tf.constant(3.)\n","g(x).numpy()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9.0"]},"metadata":{"tags":[]},"execution_count":20}]},{"metadata":{"id":"pFa2w7veDQXv","colab_type":"text"},"cell_type":"markdown","source":["Alternatively, you can can use the following "]},{"metadata":{"id":"OTNmSBfrClko","colab_type":"code","colab":{}},"cell_type":"code","source":["@tf.contrib.eager.defun\n","def s(x):\n","  return 1/(1+tf.exp(-x))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QvsSXn7_DkzG","colab_type":"code","outputId":"60a37082-3263-4ec4-bca2-945304b8ceb7","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["s(x).numpy()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.95257413"]},"metadata":{"tags":[]},"execution_count":22}]},{"metadata":{"id":"GQ8XRZhZIOBv","colab_type":"text"},"cell_type":"markdown","source":["Moreover, there is a gain in execution time when you wrap functions in in `defun`. Let us use a simple function that contains multiple layers and see the difference in time. "]},{"metadata":{"id":"oRGhYpxhDrYc","colab_type":"code","colab":{}},"cell_type":"code","source":["def encoder(x):\n","  filters = 128\n","  \n","  x = Convolution2D(filters = filters, kernel_size = 3, padding = 'same', input_shape = [224, 224, 3], activation = 'relu')(x)\n","  x = MaxPooling2D(pool_size = (2,2))(x)\n","  \n","  for i in range(0, 5):\n","    filters = filters // 2\n","    x = Convolution2D(filters = filters, kernel_size = 3, padding = 'same', activation = 'relu')(x)\n","    x = MaxPooling2D(pool_size = (2,2))(x)\n","    \n","  x = Flatten()(x)\n","  x = Dense(units = 512)(x)\n","  x = Dense(units = 1)(x)\n","  return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"txkAq0m7Fb5s","colab_type":"code","outputId":"1b6f83b4-1afa-4ba9-fb5d-47d08ca8cfad","colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["x = tf.Variable(np.zeros((10, 224, 224, 3)))\n","\n","#calculate the time in eager \n","start = time.time()\n","encoder(x)\n","print('Time in pure python',time.time()-start)\n","\n","#calculate the time in graph \n","encoder_g = tfe.defun(encoder)\n","start = time.time()\n","encoder_g(x)\n","print('Time using defun ', time.time()-start)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Time in pure python 0.1314711570739746\n","Time using defun  0.11458110809326172\n"],"name":"stdout"}]},{"metadata":{"id":"2yn5lOCuMIi-","colab_type":"text"},"cell_type":"markdown","source":["# Use Eager and Graphs \n","\n","As you can see eager is all good but can it replace graphs?  TensorFlow with graph is useful for distributed training, performance optimizations, and production/deployment. Eager execution , on the other hand, cannot be used for these purposes. So, both modes are completing each other. Use eager to debug the code, create the models with tf.keras and then use the graph execution to accelarate  functions and deploy it on other platforms. "]},{"metadata":{"id":"8oEGN5YZKUzA","colab_type":"text"},"cell_type":"markdown","source":["# Next Steps \n","\n","Take a look at the following resources to read more about eager. \n","\n","*    [Eager guide](https://www.tensorflow.org/guide/eager). \n","*   [Getting started with TensorFlow ](https://www.tensorflow.org/tutorials/)\n","*   [Real examples with seedbank](https://tools.google.com/seedbank/seeds?q=eager)\n","*   [Optimizing your code ](https://medium.com/tensorflow/code-with-eager-execution-run-with-graphs-optimizing-your-code-with-revnet-as-an-example-6162333f9b08)\n","\n"]}]}